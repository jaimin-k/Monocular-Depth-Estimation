{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from boxes import draw_border\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MiDaS model predictions on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midas = torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')\n",
    "midas.to('cuda')\n",
    "midas.eval()\n",
    "transformss = torch.hub.load('intel-isl/MiDaS', 'transforms')\n",
    "transform = transformss.small_transform\n",
    "cap = cv2.imread('snip.jpg')\n",
    "\n",
    "image = cv2.cvtColor(cap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image = cv2.resize(image,(600,500))\n",
    "image_batch = transform(image).to('cuda')\n",
    "%matplotlib widget\n",
    "with torch.no_grad():\n",
    "        prediction = midas(image_batch)\n",
    "        prediction=torch.nn.functional.interpolate(\n",
    "                    prediction.unsqueeze(1),\n",
    "                    size = image.shape[:2],\n",
    "                    mode = 'bicubic',\n",
    "                    align_corners=False\n",
    "                    ).squeeze()\n",
    "\n",
    "        output = prediction.cpu().numpy()\n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.imshow(image)\n",
    "plt.tight_layout(pad=0.0)\n",
    "plt.axis(\"off\") \n",
    "plt.subplot(1, 2,2)\n",
    "plt.imshow(output)\n",
    "plt.axis(\"off\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to use yolov7-pose model on a image frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "def image_view(imagefile, w=20, h=14):\n",
    "    \"\"\"\n",
    "    Displaying an image from an image file\n",
    "    \"\"\"\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(w, h))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(imagefile), \n",
    "                            cv2.COLOR_BGR2RGB))\n",
    "\n",
    "def loading_yolov7_model(yolomodel):\n",
    "    \"\"\"\n",
    "    Loading yolov7 model\n",
    "    \"\"\"\n",
    "    print(\"Loading model:\", yolomodel)\n",
    "    model = torch.load(yolomodel, map_location=device)['model']\n",
    "    model.float().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # half() turns predictions into float16 tensors\n",
    "        # which significantly lowers inference time\n",
    "        model.half().to(device)\n",
    "\n",
    "    return model, yolomodel\n",
    "\n",
    "\n",
    "def running_inference(image):\n",
    "    \"\"\"\n",
    "    Running yolov7 model inference\n",
    "    \"\"\"\n",
    "    image = letterbox(image, 960, \n",
    "                      stride=64,\n",
    "                      auto=True)[0]  # shape: (567, 960, 3)\n",
    "    image = transforms.ToTensor()(image)  # torch.Size([3, 567, 960])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    image = image.unsqueeze(0)  # torch.Size([1, step, 567, 960])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "\n",
    "    return output.cpu(), image\n",
    "\n",
    "def draw_keypoints(output, image, confidence=0.25, threshold=0.65):\n",
    "    \"\"\"\n",
    "    Draw YoloV7 pose keypoints\n",
    "    \"\"\"\n",
    "    output = non_max_suppression_kpt(\n",
    "        output,\n",
    "        confidence,  # Confidence Threshold\n",
    "        threshold,  # IoU Threshold\n",
    "        nc=model.yaml['nc'],  # Number of Classes\n",
    "        nkpt=model.yaml['nkpt'],  # Number of Keypoints\n",
    "        kpt_label=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = cv2.cvtColor(nimg.cpu().numpy().astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    op_pt=[]\n",
    "    for idx in range(output.shape[0]):\n",
    "        #kkp=[]\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "        xmin, ymin = (output[idx, 2]-output[idx, 4]/2), (output[idx, 3]-output[idx, 5]/2)\n",
    "        xmax, ymax = (output[idx, 2]+output[idx, 4]/2), (output[idx, 3]+output[idx, 5]/2)\n",
    "        cv2.rectangle(\n",
    "              nimg,\n",
    "              (int(xmin), int(ymin)),\n",
    "              (int(xmax), int(ymax)),\n",
    "              color=(255, 0, 0),\n",
    "              thickness=1,\n",
    "              lineType=cv2.LINE_AA\n",
    "          )\n",
    "        #kkp.append(output[idx, 7:].T)\n",
    "        op_pt = output[idx, 7:].T\n",
    "    return nimg,op_pt,output\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test yolov7-pose on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLOV7MODEL = 'yolov7-w6-pose.pt'\n",
    "\n",
    "try:\n",
    "    print(\"Loading the model...\")\n",
    "    model, yolomodel = loading_yolov7_model(yolomodel=YOLOV7MODEL)\n",
    "    print(\"Using the\", YOLOV7MODEL, \"model\")\n",
    "    print(\"Done\")\n",
    "\n",
    "except:\n",
    "    print(\"[Error] Cannot load the model\", YOLOV7MODEL)\n",
    "\n",
    "imagefile = \"test.jpg\"\n",
    "%matplotlib widget\n",
    "output1, image = running_inference(cv2.imread(imagefile))\n",
    "\n",
    "pose_image,kpts,output = draw_keypoints(output1, image, confidence=0.25, threshold=0.65)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(pose_image)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating depth of each human using MiDaS model on persons detected by Yolov7-pose model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformss = torch.hub.load('intel-isl/MiDaS', 'transforms')\n",
    "transform1 = transformss.small_transform\n",
    "\n",
    "image = cv2.imread(\"test.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "output1, image = running_inference(image)\n",
    "pose_image,kpts,output = draw_keypoints(output1, image, confidence=0.5, threshold=0.65)\n",
    "\n",
    "image = cv2.cvtColor(pose_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image_batch = transform(image).to('cuda')\n",
    "%matplotlib widget\n",
    "with torch.no_grad():\n",
    "    prediction = midas(image_batch)\n",
    "    prediction=torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size = image.shape[:2],\n",
    "            mode = 'bicubic',\n",
    "            align_corners=False\n",
    "            ).squeeze()\n",
    "\n",
    "    det = prediction.cpu().numpy()\n",
    "    boxes = []\n",
    "    inverse_depth = []\n",
    "    person=[]\n",
    "    for idx in range(output.shape[0]):\n",
    "        \n",
    "        \n",
    "        detc=0\n",
    "        plot_skeleton_kpts(det, output[idx, 7:].T, 3)\n",
    "        xmin, ymin = (output[idx, 2]-output[idx, 4]/2), (output[idx, 3]-output[idx, 5]/2)\n",
    "        xmax, ymax = (output[idx, 2]+output[idx, 4]/2), (output[idx, 3]+output[idx, 5]/2)\n",
    "\n",
    "        person.append([idx])\n",
    "        cv2.rectangle(\n",
    "                det,\n",
    "                (int(xmin), int(ymin)),\n",
    "                (int(xmax), int(ymax)),\n",
    "                color=(0,0,0),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA\n",
    "            )\n",
    "        plot_skeleton_kpts(image, output[idx, 7:].T, 3)\n",
    "        cv2.putText(det, f\"p{idx}\", (int(xmin), int(ymin-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0),2)   \n",
    "        \n",
    "        cv2.rectangle(\n",
    "                image,\n",
    "                (int(xmin), int(ymin)),\n",
    "                (int(xmax), int(ymax)),\n",
    "                color=(255,255,255),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA\n",
    "            )\n",
    "        \n",
    "        cv2.putText(image, f\"p{idx}\", (int(xmin), int(ymin-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255),2)   \n",
    "        \n",
    "        \n",
    "        box = [int(xmin),int(ymin),int(xmax),int(ymax)]\n",
    "        boxes.append(box)\n",
    "        \n",
    "        inv_depth = det[boxes[idx][1]:boxes[idx][3],boxes[idx][0]:boxes[idx][2]].mean()\n",
    "        inverse_depth.append(inv_depth)\n",
    "        if (inv_depth/1000)<0.25:\n",
    "            cv2.putText(det, f'far', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (0, 255, 25), 2)\n",
    "            cv2.putText(image, f'far', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (0, 255, 25), 2)\n",
    "        elif (inv_depth/1000)>0.25:\n",
    "                cv2.putText(det, f'near!', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (255, 0, 0), 3)\n",
    "                cv2.putText(image, f'near!', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (255, 0, 0), 3)\n",
    "\n",
    "        #cv2.putText(det, f'proximity:{(inv_depth)*0.001:.2f}', (int(xmin-20), int(ymax+10)), cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "                        #(255, 255, 0), 2)\n",
    "        #cv2.putText(image, f'proximity:{inv_depth*0.001:.2f}', (int(xmin-20), int(ymax+10)), cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "                        #(255, 255, 0), 2)\n",
    "        \n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.imshow(det)\n",
    "    plt.tight_layout(pad=0.0)\n",
    "    plt.axis(\"off\") \n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\") \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the depth detections data into a .csv and .txt data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ped = np.array(ped)\n",
    "boxes = np.array(boxes)\n",
    "inverse_depth = np.array(inverse_depth)\n",
    "np.set_printoptions(suppress=True)\n",
    "inverse_depth =inverse_depth.reshape(-1,1)\n",
    "\n",
    "merged_array = np.concatenate((person, boxes[:,0:3], inverse_depth), axis=1)\n",
    "\n",
    "# Write to a CSV file\n",
    "np.savetxt(\"Depth_data.csv\", merged_array, delimiter=\",\",header=\"person,x1,y1,x2,y2,depth\",fmt='%f')\n",
    "\n",
    "# Write to a TXT file\n",
    "np.savetxt(\"Depth_data.txt\", merged_array, delimiter=\"\\t\",header=\"person,x1,y1,x2,y2,depth\",fmt='%f')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformss = torch.hub.load('intel-isl/MiDaS', 'transforms')\n",
    "transform1 = transformss.small_transform\n",
    "#inverse_depth=[]\n",
    "cap = cv2.VideoCapture('test1.mp4')\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    output1, image = running_inference(frame)\n",
    "    pose_image,kpts,output = draw_keypoints(output1, image, confidence=0.25, threshold=0.65)\n",
    "    \n",
    "    image = cv2.cvtColor(pose_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_batch = transform(image).to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    prediction = midas(image_batch)\n",
    "    prediction=torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size = image.shape[:2],\n",
    "            mode = 'bicubic',\n",
    "            align_corners=False\n",
    "            ).squeeze()\n",
    "\n",
    "    det = prediction.cpu().numpy()\n",
    "    boxes = []\n",
    "    inverse_depth = []\n",
    "    person=[]\n",
    "    for idx in range(output.shape[0]):\n",
    "        \n",
    "        \n",
    "        detc=0\n",
    "        plot_skeleton_kpts(det, output[idx, 7:].T, 3)\n",
    "        xmin, ymin = (output[idx, 2]-output[idx, 4]/2), (output[idx, 3]-output[idx, 5]/2)\n",
    "        xmax, ymax = (output[idx, 2]+output[idx, 4]/2), (output[idx, 3]+output[idx, 5]/2)\n",
    "\n",
    "        person.append([idx])\n",
    "        cv2.rectangle(\n",
    "                det,\n",
    "                (int(xmin), int(ymin)),\n",
    "                (int(xmax), int(ymax)),\n",
    "                color=(0,0,0),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA\n",
    "            )\n",
    "        plot_skeleton_kpts(image, output[idx, 7:].T, 3)\n",
    "        cv2.putText(det, f\"p{idx}\", (int(xmin), int(ymin-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0),)   \n",
    "        \n",
    "        cv2.rectangle(\n",
    "                image,\n",
    "                (int(xmin), int(ymin)),\n",
    "                (int(xmax), int(ymax)),\n",
    "                color=(255,255,255),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA\n",
    "            )\n",
    "        \n",
    "        cv2.putText(image, f\"p{idx}\", (int(xmin), int(ymin-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255),2)   \n",
    "        \n",
    "        \n",
    "        box = [int(xmin),int(ymin),int(xmax),int(ymax)]\n",
    "        boxes.append(box)\n",
    "        \n",
    "        inv_depth = det[boxes[idx][1]:boxes[idx][3],boxes[idx][0]:boxes[idx][2]].mean()\n",
    "        inverse_depth.append(inv_depth)\n",
    "        if (inv_depth/1000)<0.25:\n",
    "            cv2.putText(det, f'far', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (0, 255, 25), 2)\n",
    "            cv2.putText(image, f'far', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (0, 255, 25), 2)\n",
    "        elif (inv_depth/1000)>0.25:\n",
    "                cv2.putText(det, f'near!', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (255, 0, 0), 3)\n",
    "                cv2.putText(image, f'near!', (int(xmin), int(ymax-25)), cv2.FONT_HERSHEY_PLAIN, 2,\n",
    "                        (255, 0, 0), 3)\n",
    "\n",
    "        cv2.putText(det, f'proximity:{1.2*inv_depth/1000:.2f}', (int(xmin-20), int(ymax+25)), cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "                        (255, 255, 0), 2)\n",
    "        cv2.putText(image, f'proximity:{1.2*inv_depth/1000:.2f}', (int(xmin-20), int(ymax+25)), cv2.FONT_HERSHEY_PLAIN, 1.5,\n",
    "                        (255, 255, 0), 2)\n",
    "    cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(\"img\",frame)\n",
    "    cv2.resizeWindow(\"img\",720,440)\n",
    "    plt.imshow(det)\n",
    "\n",
    "    #plt.imshow(frame)\n",
    " \n",
    "    plt.pause(0.00001)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on a webcam feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image_batch = transform(image).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(image_batch)\n",
    "        prediction=torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size = image.shape[:2],\n",
    "            mode = 'bicubic',\n",
    "            align_corners=False\n",
    "            ).squeeze()\n",
    "\n",
    "        output = prediction.cpu().numpy()\n",
    "\n",
    "        #print(output)\n",
    "    plt.imshow(output)\n",
    "    #plt.imshow(frame)\n",
    "    cv2.imshow('CV2Frame',frame)\n",
    "    plt.pause(0.0001)\n",
    "    print(prediction)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb94555b3bad7502515e9aa7677dbe7f2dab987ecd902c9fcfd0ed5fef3b5a39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
